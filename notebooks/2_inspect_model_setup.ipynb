{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30550d05-6ecd-45d9-aae9-beef48d3400d",
   "metadata": {},
   "source": [
    "# BERT models for swedish sentence classification\n",
    "The present notebook briefly presents the models used for sentence classification.\n",
    "Training is run by subsequent notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b58f74-807b-4429-a1b1-21547e644e24",
   "metadata": {},
   "source": [
    "## The models\n",
    "The main model is a BERT language model pretrained on swedish.\n",
    "It is the first-appearing result deemed suitable in [this search for models pre-trained on swedish](https://huggingface.co/models?language=sv&sort=downloads&search=bert+swedish).\n",
    "\n",
    "The second model is based on the former, but is set up for fine-tuning with LoRA.\n",
    "This choice was based on Edvard's personal fascination and interest in the technique, as it\n",
    "- is often, due to memory constraints, the only feasible option for training models beyond a certain size\n",
    "- can save a lot of memory if fine-tuning many models using the same base\n",
    "- is based on a beautifully simple idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8977396f-311a-47ed-be90-739010cd5ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m swedish_classifier.model() -> transformers.models.bert.modeling_bert.BertForSequenceClassification\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m model() -> BertForSequenceClassification:\n",
       "    \u001b[33m\"\"\"BERT-based swedish classifier for fine-tuning.\"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m BertForSequenceClassification.from_pretrained(\n",
       "        \u001b[33m\"KB/bert-base-swedish-cased\"\u001b[39m, num_labels=\u001b[32m2\u001b[39m\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      ~/model-deployment-starter/models/swedish_classifier.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models import swedish_classifier\n",
    "\n",
    "swedish_classifier.model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "988992c8-77fc-4cc7-8b70-b09ae4ae79c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mSignature:\u001b[39m swedish_classifier.lora_model(lora_r: int = \u001b[32m4\u001b[39m) -> peft.peft_model.PeftModel\n",
       "\u001b[31mSource:\u001b[39m   \n",
       "\u001b[38;5;28;01mdef\u001b[39;00m lora_model(lora_r: int = \u001b[32m4\u001b[39m) -> PeftModel:\n",
       "    \u001b[33m\"\"\"BERT-based swedish classifier configured for LoRA fine-tuning.\"\"\"\u001b[39m\n",
       "    \u001b[38;5;28;01mreturn\u001b[39;00m get_peft_model(\n",
       "        model(),\n",
       "        LoraConfig(\n",
       "            \u001b[33m\"SEQ_CLS\"\u001b[39m,\n",
       "            target_modules=[\u001b[33m\"query\"\u001b[39m, \u001b[33m\"value\"\u001b[39m],\n",
       "            r=lora_r,\n",
       "            lora_alpha=\u001b[32m2\u001b[39m * lora_r,\n",
       "            lora_dropout=\u001b[32m0.2\u001b[39m,\n",
       "            modules_to_save=[\u001b[33m\"classifier\"\u001b[39m],\n",
       "        )\n",
       "    )\n",
       "\u001b[31mFile:\u001b[39m      ~/model-deployment-starter/models/swedish_classifier.py\n",
       "\u001b[31mType:\u001b[39m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "swedish_classifier.lora_model??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7f102-763d-4415-a5a7-01a09a097138",
   "metadata": {},
   "source": [
    "## Training notes\n",
    "The training routine implemented in `models.swedish_classifier.train` implements early stopping using the criterion of two epochs without validation loss improvement.\n",
    "\n",
    "Below are some notes on hyperparameter settings.\n",
    "- Batch size is left to `transformer`'s default value of 8.\n",
    "- Dropout probability in the main model is left to the default 0.1.\n",
    "- Learn rates were experimentally set as large as possible while still showing decreasing validation loss in the initial epochs, which turned out to mean `1e-5` for the main model and `1e-4` for the LoRA one.\n",
    "- For LoRA rank, the [original paper](https://arxiv.org/pdf/2106.09685) shows anything between 1 and 64 to be effective. The present code uses 4.\n",
    "- For LoRA alpha, `2 * lora_r` is [common practice](https://unsloth.ai/docs/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n",
    "- LoRA dropout probability is slightly higher than default, 0.2, based on a hunch that the increased regularization is useful given the small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0476d6-029c-46be-9b48-671c7907b891",
   "metadata": {},
   "source": [
    "## % of parameters tuned in the LoRA model\n",
    "The LoRA setup tunes only 0.12% of the parameters, nearly eliminating the training memory requirements (which are about 3X the size of the model itself).\n",
    "Look for \"lora_A\" and \"lora_B\" in the layers printed at the bottom of this notebook to understand how the fraction can be so small.\n",
    "While fine-tuning optimizes all layers, with sizes such as $768 \\times 768$ and $3072 \\times 768$ (1'000'000s of parameters), the LoRA setup optimizes a subset of replacement layers of size $768 \\times 4$ (1'000s of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671545d6-9711-42de-9faf-09dc6ef945f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 124,841,476 || trainable%: 0.1193\n"
     ]
    }
   ],
   "source": [
    "swedish_classifier.lora_model().print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed03d22-8142-4815-9b2a-540018ef2815",
   "metadata": {},
   "source": [
    "## Neural network layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea166ef-fc24-4bc1-a997-13a35b252306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50325, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swedish_classifier.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e27079c-58e9-42cc-99a0-702e98738fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BertForSequenceClassification(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(50325, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSdpaSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (pooler): BertPooler(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (activation): Tanh()\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swedish_classifier.lora_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
